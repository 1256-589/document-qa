import streamlit as st
from PyPDF2 import PdfReader
from langchain import hub
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.agents import AgentAction
from langchain_core.callbacks import BaseCallbackHandler
from langchain_community.vectorstores import FAISS
from langchain.tools.retriever import create_retriever_tool
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_community.embeddings import DashScopeEmbeddings
from langchain.chat_models import init_chat_model
from langchain_community.tools import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
from langchain_core.messages import AIMessage, HumanMessage
import os
from dotenv import load_dotenv
import shutil
import time
import pickle
from datetime import datetime

# --- åˆå§‹åŒ–ä¸é…ç½® ---
load_dotenv(override=True)
st.set_page_config(page_title="AI ç ”ç©¶åŠ©æ‰‹", page_icon="ğŸ’¡")

# --- å¸¸é‡å®šä¹‰ ---
DB_DIR = "document_database"
FILES_DIR = os.path.join(DB_DIR, "files")
INDEX_PATH = os.path.join(DB_DIR, "faiss_index")
CHAT_HISTORY_DIR = "chat_history"
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


# --- åç«¯è¾…åŠ©å‡½æ•° ---
@st.cache_resource
def get_embeddings():
    return DashScopeEmbeddings(model="text-embedding-v2", dashscope_api_key=os.getenv("DASHSCOPE_API_KEY"))


embeddings = get_embeddings()


def setup_directories():
    os.makedirs(FILES_DIR, exist_ok=True)
    os.makedirs(CHAT_HISTORY_DIR, exist_ok=True)


# --- æ•°æ®å­˜å‚¨å‡½æ•° (é‡å¤§ä¿®æ”¹) ---
def save_chat_history(title, messages, file_path):
    """ä¿å­˜å¯¹è¯æ ‡é¢˜å’Œæ¶ˆæ¯åˆ° Pickle æ–‡ä»¶"""
    if not file_path:
        st.error("é”™è¯¯ï¼šæ²¡æœ‰æä¾›æœ‰æ•ˆçš„å¯¹è¯æ–‡ä»¶è·¯å¾„ç”¨äºä¿å­˜ï¼")
        return
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, "wb") as f:
        pickle.dump({"title": title, "messages": messages}, f)


def load_chat_history(file_path):
    """ä» Pickle æ–‡ä»¶åŠ è½½å¯¹è¯ï¼Œè¿”å› (æ ‡é¢˜, æ¶ˆæ¯åˆ—è¡¨)"""
    if os.path.exists(file_path):
        try:
            with open(file_path, "rb") as f:
                data = pickle.load(f)
                # å…¼å®¹æ–°æ—§ä¸¤ç§æ•°æ®æ ¼å¼
                if isinstance(data, dict):
                    return data.get("title"), data.get("messages", [])
                elif isinstance(data, list):
                    return None, data  # æ—§æ ¼å¼ï¼Œæ²¡æœ‰æ ‡é¢˜
        except (pickle.UnpicklingError, EOFError):
            return "æŸåçš„å¯¹è¯", [AIMessage(content="æ— æ³•åŠ è½½å¯¹è¯ï¼Œæ–‡ä»¶å¯èƒ½å·²æŸåã€‚")]
    return None, []


def get_chat_title_from_file(file_path):
    """åªè¯»å–æ ‡é¢˜ï¼Œç”¨äºåœ¨ä¾§è¾¹æ å¿«é€Ÿæ˜¾ç¤ºï¼Œé¿å…åŠ è½½æ•´ä¸ªå¯¹è¯å†å²"""
    title, _ = load_chat_history(file_path)
    return title


# --- LLM ç›¸å…³å‡½æ•° ---
def generate_chat_title(query):
    """è°ƒç”¨LLMä¸ºå¯¹è¯ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜"""
    st.toast("æ­£åœ¨ä¸ºæ–°å¯¹è¯ç”Ÿæˆæ ‡é¢˜...", icon="ğŸ·ï¸")
    title_prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬æ‘˜è¦åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„æé—®ï¼Œä¸ºè¿™ä¸ªå¯¹è¯ç”Ÿæˆä¸€ä¸ªä¸è¶…è¿‡8ä¸ªå­—çš„ã€ç®€çŸ­ç²¾ç‚¼çš„æ ‡é¢˜ã€‚"),
        ("user", "{query}")
    ])
    # ä¸ºäº†èŠ‚çœæˆæœ¬å’Œæé«˜é€Ÿåº¦ï¼Œå¯ä»¥ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹
    llm = init_chat_model("deepseek-chat", model_provider="deepseek", temperature=0.1)
    chain = title_prompt | llm
    try:
        response = chain.invoke({"query": query})
        return response.content.strip().replace("\"", "")
    except Exception as e:
        print(f"æ ‡é¢˜ç”Ÿæˆå¤±è´¥: {e}")
        return "æ–°å¯¹è¯"


# ... (process_and_index_files, get_relevant_files, get_agent_executor, run_agent_with_streaming å‡½æ•°ä¿æŒä¸å˜) ...
def process_and_index_files(pdf_docs, progress_placeholder):
    """å¤„ç†ä¸Šä¼ çš„PDFæ–‡ä»¶ï¼Œä¿å­˜ã€åˆ‡åˆ†å¹¶åˆ›å»ºæˆ–æ›´æ–°å‘é‡ç´¢å¼•"""
    texts_with_metadata = []
    progress_bar = progress_placeholder.progress(0, text="å¼€å§‹å¤„ç†æ–‡ä»¶...")

    for i, pdf in enumerate(pdf_docs):
        file_path = os.path.join(FILES_DIR, pdf.name)
        with open(file_path, "wb") as f:
            f.write(pdf.getbuffer())

        pdf_reader = PdfReader(file_path)
        for page in pdf_reader.pages:
            text = page.extract_text()
            if text:
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                chunks = text_splitter.split_text(text)
                for chunk in chunks:
                    texts_with_metadata.append((chunk, {"source": pdf.name}))

        progress_bar.progress((i + 1) / len(pdf_docs), text=f"æ­£åœ¨å¤„ç†: {pdf.name}")

    if not texts_with_metadata:
        progress_bar.empty()
        return 0

    texts = [item[0] for item in texts_with_metadata]
    metadatas = [item[1] for item in texts_with_metadata]

    if os.path.exists(INDEX_PATH):
        db = FAISS.load_local(INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
        db.add_texts(texts, metadatas=metadatas)
    else:
        db = FAISS.from_texts(texts, embeddings, metadatas=metadatas)

    db.save_local(INDEX_PATH)
    progress_bar.progress(1.0, text="æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")
    return len(pdf_docs)


def get_relevant_files(query):
    """æ ¹æ®æŸ¥è¯¢åœ¨æ•´ä¸ªæ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡ä»¶å"""
    if not os.path.exists(INDEX_PATH):
        return []

    db = FAISS.load_local(INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
    retriever = db.as_retriever(search_kwargs={'k': 20})
    relevant_docs = retriever.get_relevant_documents(query)

    return list(set(doc.metadata['source'] for doc in relevant_docs))


def get_agent_executor(selected_files=None):
    """åˆ›å»ºå¸¦æœ‰å·¥å…·çš„ Agent Executorã€‚å¦‚æœé€‰æ‹©äº†æ–‡ä»¶ï¼Œåˆ™æ–‡æ¡£æ£€ç´¢å™¨ä¼šè¢«è¿‡æ»¤"""
    search = TavilySearchResults(max_results=6)

    @tool
    def search_tool(query: str) -> str:
        """å½“ä½ éœ€è¦æœç´¢æœ€æ–°ä¿¡æ¯æˆ–ç”¨æˆ·é—®é¢˜ä¸æœ¬åœ°æ–‡æ¡£æ— å…³æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚"""
        return search.invoke(query)

    tools = [search_tool]

    if selected_files and os.path.exists(INDEX_PATH):
        full_db = FAISS.load_local(INDEX_PATH, embeddings, allow_dangerous_deserialization=True)

        filtered_docs = []
        for doc_id, doc in full_db.docstore._dict.items():
            if doc.metadata.get("source") in selected_files:
                filtered_docs.append(doc)

        if filtered_docs:
            filtered_db = FAISS.from_documents(filtered_docs, embeddings)
            retriever = filtered_db.as_retriever(search_kwargs={'k': 10})  # å¢åŠ æ£€ç´¢æ•°é‡

            document_retriever_tool = create_retriever_tool(
                retriever,
                "document_retriever",
                f"ä¼˜å…ˆä½¿ç”¨æ­¤å·¥å…·å›ç­”å…³äºç‰¹å®šæ–‡æ¡£çš„é—®é¢˜ã€‚è¿™äº›æ˜¯ç”¨æˆ·æˆæƒä½ æŸ¥é˜…çš„æ–‡æ¡£ï¼š{', '.join(selected_files)}ã€‚"
            )
            tools.append(document_retriever_tool)

    llm = init_chat_model("deepseek-chat", model_provider="deepseek", temperature=0.7)
    prompt = hub.pull("hwchase17/openai-tools-agent")

    agent = create_openai_tools_agent(llm, tools, prompt)
    return AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)


def run_agent_with_streaming(agent_executor, query, history):
    """è¿è¡Œ Agent å¹¶é€šè¿‡å›è°ƒå®ç°æµå¼è¾“å‡º"""

    class StreamingCallbackHandler(BaseCallbackHandler):
        def __init__(self, placeholder):
            self.placeholder = placeholder
            self.full_response = ""

        def on_llm_new_token(self, token: str, **kwargs: any) -> None:
            self.full_response += token
            self.placeholder.markdown(self.full_response + "â–Œ")  # ä½¿ç”¨å…‰æ ‡æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ

        def on_agent_action(self, action: AgentAction, **kwargs: any) -> any:
            tool_name = action.tool
            self.placeholder.markdown(f"æ­£åœ¨è°ƒç”¨å·¥å…·: `{tool_name}`...")

    placeholder = st.empty()
    streaming_handler = StreamingCallbackHandler(placeholder)

    response = agent_executor.invoke(
        {"input": query, "chat_history": history},
        config={"callbacks": [streaming_handler]}
    )

    output = response.get('output', 'å¤„ç†å‡ºé”™ï¼Œè¯·é‡è¯•ã€‚')
    placeholder.markdown(output)
    return output


# --- ä¸»ç¨‹åºç•Œé¢ ---
def start_new_chat():
    """åˆ›å»ºä¸€ä¸ªæ–°çš„å¯¹è¯ï¼Œå¹¶ç«‹å³æŒä¹…åŒ–"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    new_chat_filename = f"chat_{timestamp}.pkl"
    st.session_state.active_chat_file_path = os.path.join(CHAT_HISTORY_DIR, new_chat_filename)

    st.session_state.active_chat_title = None
    st.session_state.chat_history = [AIMessage(content="æ‚¨å¥½ï¼æ–°çš„å¯¹è¯å·²å¼€å¯ã€‚")]
    st.session_state.file_selection = {"show": False, "query": None, "options": []}

    save_chat_history(
        st.session_state.active_chat_title,
        st.session_state.chat_history,
        st.session_state.active_chat_file_path
    )
    st.rerun()


def main():
    setup_directories()

    def load_css(file_name):
        with open(file_name, "r", encoding="utf-8") as f:
            st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

    load_css('style.css')

    # --- ä¼šè¯çŠ¶æ€åˆå§‹åŒ– ---
    if "chat_history" not in st.session_state:
        chat_files = sorted(
            [f for f in os.listdir(CHAT_HISTORY_DIR) if f.startswith("chat_") and f.endswith(".pkl")],
            reverse=True
        )
        if chat_files:
            latest_chat_file = chat_files[0]
            st.session_state.active_chat_file_path = os.path.join(CHAT_HISTORY_DIR, latest_chat_file)
            title, messages = load_chat_history(st.session_state.active_chat_file_path)
            st.session_state.active_chat_title = title
            st.session_state.chat_history = messages
        else:
            # é¦–æ¬¡è¿è¡Œæ—¶åˆ›å»ºç¬¬ä¸€ä¸ªå¯¹è¯
            start_new_chat()
            # st.rerun() ä¼šå¯¼è‡´ Streamlit é‡æ–°æ‰§è¡Œï¼Œæ‰€ä»¥è¿™é‡Œä¸éœ€è¦å†å†™ä»£ç 

    # --- ä¾§è¾¹æ  UI ---
    with st.sidebar:
        # st.divider()

        u1, u2 = st.columns([0.7, 0.3])
        with u1:
            st.title("RAG Agent")
        with u2:
            st.session_state.rag_active = st.toggle("RAG", value=False,label_visibility="collapsed")

        # å½“å¯¹è¯å†å²>1æ¡æ—¶ï¼ˆå³ç”¨æˆ·å·²æé—®ï¼‰ï¼Œæ‰å…è®¸æ–°å»ºå¯¹è¯
        is_chatting = len(st.session_state.get("chat_history", [])) > 1
        if st.button("ğŸ“ æ–°å»ºå¯¹è¯", use_container_width=True, disabled=not is_chatting,type="primary"):
            start_new_chat()
        st.divider()
        st.subheader("ğŸ—“ï¸ å¯¹è¯å†å²")
        all_chats = sorted(
            [f for f in os.listdir(CHAT_HISTORY_DIR) if f.startswith("chat_") and f.endswith(".pkl")],
            reverse=True
        )
        st.divider()

        for chat_file in all_chats:
            file_path = os.path.join(CHAT_HISTORY_DIR, chat_file)
            title = get_chat_title_from_file(file_path)

            if not title:
                ts_str = chat_file.replace("chat_", "").replace(".pkl", "")
                try:
                    title = f"å¯¹è¯äº {datetime.strptime(ts_str, '%Y%m%d_%H%M%S').strftime('%y-%m-%d %H:%M')}"
                except ValueError:
                    title = "æœªçŸ¥å¯¹è¯"

            is_active = file_path == st.session_state.get("active_chat_file_path")
            label = f"**{title}**" if is_active else title

            if st.button(label, key=chat_file, use_container_width=True):
                st.session_state.active_chat_file_path = file_path
                title, messages = load_chat_history(file_path)
                st.session_state.active_chat_title = title
                st.session_state.chat_history = messages
                st.rerun()
        st.divider()

        pdf_docs = st.file_uploader("ä¸Šä¼ æ–°çš„ PDF æ–‡æ¡£", accept_multiple_files=True, type=['pdf'],label_visibility="collapsed")

        if st.button("â–¶ï¸ å¼€å§‹å¤„ç†", disabled=not pdf_docs, use_container_width=True,type="primary"):
            progress_placeholder = st.empty()
            with st.spinner("æ­£åœ¨å¤„ç†å’Œç´¢å¼•æ–‡æ¡£..."):
                count = process_and_index_files(pdf_docs, progress_placeholder)
            st.success(f"æˆåŠŸå¤„ç†å¹¶ç´¢å¼•äº† {count} ä¸ªæ–°æ–‡ä»¶ã€‚")
            time.sleep(2)
            progress_placeholder.empty()
            st.rerun()
        st.divider()

        with st.expander("ğŸ“š æ–‡æ¡£åº“", expanded=True):

            indexed_files = [f for f in os.listdir(FILES_DIR) if f.endswith('.pdf')] if os.path.exists(
                    FILES_DIR) else []
            if indexed_files:
                st.caption(f"å½“å‰åº“ä¸­æœ‰ {len(indexed_files)} ä¸ªæ–‡æ¡£")
                for file_name in indexed_files:
                    st.markdown(f"ğŸ“„ {file_name}")
            else:
                st.info("æ–‡æ¡£åº“ä¸ºç©ºã€‚")

        st.divider()
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ®", use_container_width=True,type="primary"):
            # ... UIæ— å˜åŒ–
            # if st.checkbox("ç¡®è®¤è¦åˆ é™¤æ‰€æœ‰æ–‡æ¡£å’ŒèŠå¤©è®°å½•å—ï¼Ÿ"):
            if os.path.exists(DB_DIR): shutil.rmtree(DB_DIR)
            if os.path.exists(CHAT_HISTORY_DIR): shutil.rmtree(CHAT_HISTORY_DIR)
            st.session_state.clear()
            st.success("æ‰€æœ‰æ•°æ®å·²è¢«æ¸…ç©ºã€‚åº”ç”¨å°†åœ¨ä¸€ç§’ååˆ·æ–°ã€‚")
            time.sleep(1)
            st.rerun()

    # --- ä¸»èŠå¤©ç•Œé¢ ---
    for message in st.session_state.get("chat_history", []):
        avatar = "ğŸ’¡" if isinstance(message, AIMessage) else "ğŸ§‘â€ğŸ’»"
        with st.chat_message(message.type, avatar=avatar):
            st.write(message.content)

    if st.session_state.get("file_selection", {}).get("show"):
        # ... RAGé€‰æ‹©ç•Œé¢æ— å˜åŒ–
        with st.chat_message("ai", avatar="ğŸ’¡"):
            st.info("æ ¹æ®æ‚¨çš„é—®é¢˜ï¼Œæˆ‘æ‰¾åˆ°äº†è¿™äº›å¯èƒ½ç›¸å…³çš„æ–‡æ¡£ã€‚è¯·é€‰æ‹©æ‚¨å¸Œæœ›æˆ‘é‡ç‚¹æŸ¥é˜…çš„æ–‡ä»¶ï¼š")

            selected_files = st.multiselect(
                "é€‰æ‹©æ–‡æ¡£:",
                options=st.session_state.file_selection["options"],
                default=st.session_state.file_selection["options"],
                label_visibility="collapsed"
            )

            if st.button("åŸºäºæ‰€é€‰æ–‡ä»¶ç”Ÿæˆå›ç­”", type="primary"):
                query = st.session_state.file_selection["query"]
                # st.session_state.chat_history.append(HumanMessage(content=query))

                with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
                    st.write(query)

                with st.chat_message("ai", avatar="ğŸ’¡"):
                    with st.spinner("æ­£åœ¨åŸºäºæ–‡æ¡£æ€è€ƒ..."):
                        agent_executor = get_agent_executor(selected_files)
                        final_query = f"è¯·ä¸¥æ ¼ä¸”ä¼˜å…ˆåŸºäºæä¾›çš„ `document_retriever` å·¥å…·æ¥å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚é—®é¢˜æ˜¯ï¼š{query}"
                        response_content = run_agent_with_streaming(agent_executor, final_query,
                                                                    st.session_state.chat_history[1:-1])

                st.session_state.chat_history.append(AIMessage(content=response_content))
                # å®æ—¶ä¿å­˜åˆ°å½“å‰æ¿€æ´»çš„å¯¹è¯æ–‡ä»¶ä¸­
                save_chat_history(st.session_state.active_chat_title, st.session_state.chat_history,
                                  st.session_state.active_chat_file_path)
                st.session_state.file_selection["show"] = False
                st.rerun()

    if user_query := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        # å°†ç”¨æˆ·é—®é¢˜æ·»åŠ åˆ°å†å²è®°å½•
        st.session_state.chat_history.append(HumanMessage(content=user_query))

        # --- æ ‡é¢˜ç”Ÿæˆé€»è¾‘ ---
        # å¦‚æœæ˜¯é¦–æ¬¡æé—® (å†å²è®°å½•ä¸º2æ¡) ä¸”å½“å‰å¯¹è¯æ²¡æœ‰æ ‡é¢˜

        # æ­£å¸¸ç»§ç»­AIå›å¤æµç¨‹
        with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
            st.write(user_query)
        if len(st.session_state.chat_history) == 2 and st.session_state.active_chat_title is None:
            new_title = generate_chat_title(user_query)
            st.session_state.active_chat_title = new_title
            # ç«‹å³ä¿å­˜æ–°æ ‡é¢˜
            save_chat_history(
                st.session_state.active_chat_title,
                st.session_state.chat_history,
                st.session_state.active_chat_file_path
            )

        # RAG or Web Search
        if st.session_state.rag_active and os.path.exists(INDEX_PATH):
            relevant_files = get_relevant_files(user_query)
            if relevant_files:
                st.session_state.file_selection = {"show": True, "query": user_query, "options": relevant_files}
                st.rerun()

        if not st.session_state.get("file_selection", {}).get("show"):
            with st.chat_message("ai", avatar="ğŸ’¡"):
                with st.spinner("æ€è€ƒä¸­..."):
                    agent_executor = get_agent_executor()
                    response_content = run_agent_with_streaming(agent_executor, user_query,
                                                                st.session_state.chat_history[1:-1])

            st.session_state.chat_history.append(AIMessage(content=response_content))
            # ä¿å­˜åŒ…å«AIå›å¤çš„å®Œæ•´å¯¹è¯
            save_chat_history(
                st.session_state.active_chat_title,
                st.session_state.chat_history,
                st.session_state.active_chat_file_path
            )
            st.rerun()


if __name__ == "__main__":
    main()
